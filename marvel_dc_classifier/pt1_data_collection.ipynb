{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8796fd-b48e-4d51-862d-cf876a86a448",
   "metadata": {},
   "source": [
    "# Classifying Reddit Posts to Marvel/DC Movie Subreddits Pt 1 \n",
    "\n",
    "## 1. Problem Statement\n",
    "We are employees of a marketing agency hired by a toy company to perform market research on Reddit to build a classifier model that classifies posts from Marvel vs DC movies subreddits in order to\n",
    "\n",
    "Build a classifier model that can be applied to other platforms (e.g. Twitter, Facebook) with text data to determine public interest in either movie franchise\n",
    "Find popular heroes and keywords for each franchise to identify product and marketing opportunities (using sentiment analysis)\n",
    "The success of the classifier model will be evaluated using the accuracy metric i.e. is the model able to correctly label a post as coming from the Marvel/DC subreddit? Similarly, sentiment analysis will be evaluated using accuracy i.e. is the model able to correctly identify a post as having positive, neutral or negative sentiment? This can help us identify the most discussed heroes and keywords (a sign of popularity) and whether the discussions around these heroes and topics are positive, neutral or negative to identify product and marketing opportunities to boost revenue for our customer, the toy company.\n",
    "\n",
    "## 2. Overview of notebook\n",
    "In this first code notebook, we use the Pushshift API to get posts from the Marvel and DC subreddits to be used as the training dataset for our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a07437-5a2e-409f-a858-12efe648cdef",
   "metadata": {},
   "source": [
    "## 3. Import libraries\n",
    "\n",
    "All libraries used will be imported here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d43b549-d852-43d0-9b19-edd79348a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdbd492-e4ca-4df2-85aa-cd21a23519d5",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Use the API to collect sufficient data for NLP and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b664579-e3fd-4da5-9498-5897a7f8cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes in a subreddit and number of posts (in multiples of 100)\n",
    "# and returns a DataFrame including the subreddit, title, selftext and creation time of the post \n",
    "\n",
    "def get_posts(subreddit, n):\n",
    "    # x = 0 \n",
    "    posts = []\n",
    "    date = round(time.time()) # current time\n",
    "    \n",
    "    for x in range(1,(n/100)+1): \n",
    "        # Call API to pull data \n",
    "        url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "        params = {\n",
    "        'subreddit': subreddit\n",
    "        ,'size': 100\n",
    "        ,'before': date\n",
    "        }\n",
    "        res = requests.get(url, params)\n",
    "        data = res.json()\n",
    "        temp_df = pd.DataFrame(data['data'])\n",
    "        \n",
    "        # Add onto/reset accumulators \n",
    "        date = min(temp_df['created_utc']) # get the min created utc from the most recent loop of data pulled\n",
    "        # x += len(data['data'])\n",
    "        posts.extend(data['data']) # data['data'] is a list of posts, each post is a dict \n",
    "        \n",
    "        #Track progress\n",
    "        # print(x)\n",
    "        \n",
    "        # Wait 1-3 min before calling API again\n",
    "        # time.sleep(randint(60,180)) \n",
    "        \n",
    "    # Return a dataframe of all the posts \n",
    "    return pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9edec4-e2cc-4436-8013-0c4f35ab4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function to collect and store DC subreddit data in dataframes\n",
    "dc_df = get_posts('DC_Cinematic', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca1fa0a-6a8b-4efe-8b4c-3c147607e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function to collect and store marvel subreddit data in dataframes \n",
    "marvel_df = get_posts('marvelstudios', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58d4f70-cb07-43b1-bb07-a8afd75abc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes in csv files \n",
    "marvel_df.to_csv('./datasets/marvel.csv', index=False)\n",
    "dc_df.to_csv('./datasets/dc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
